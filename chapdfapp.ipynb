{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-core langchain-community colab-xterm pypdf ollama transformers streamlit torch numpy scikit-learn\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# !ollama serve & ollama run llama3\n",
    "\n",
    "%load_ext colabxterm\n",
    "%xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from typing import Dict, List\n",
    "from pypdf import PdfReader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "# Initializing embedding tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "#function to parse the uploaded files: PDF, TXT, MD\n",
    "def parse_folder(files: List[BytesIO], filenames: List[str]) -> Dict[str, List[str]]:\n",
    "    def parse_pdf(file: BytesIO) -> List[str]:\n",
    "        pdf = PdfReader(file)\n",
    "        output = []\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "            text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "            text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "            output.append(text)\n",
    "        return output\n",
    "\n",
    "    def parse_text(file: BytesIO) -> List[str]:\n",
    "        output = []\n",
    "        text = file.read().decode('utf-8')\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "        output.append(text)\n",
    "        return output\n",
    "\n",
    "    parsed_files = {}\n",
    "    for file, filename in zip(files, filenames):\n",
    "        if filename.endswith('.pdf'):\n",
    "            parsed_files[filename] = parse_pdf(file)\n",
    "        elif filename.endswith('.md') or filename.endswith('.txt'):\n",
    "            parsed_files[filename] = parse_text(file)\n",
    "\n",
    "    return parsed_files\n",
    "\n",
    "#function to conver text into chunks of size=2000\n",
    "def split_text_into_chunks(parsed_files: Dict[str, List[str]], chunk_size=2000, chunk_overlap=0) -> Dict[str, List[str]]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "\n",
    "    chunks = {}\n",
    "    for filename, texts in parsed_files.items():\n",
    "        file_chunks = []\n",
    "        for text in texts:\n",
    "            file_chunks.extend(text_splitter.split_text(text))\n",
    "        chunks[filename] = file_chunks\n",
    "\n",
    "    return chunks\n",
    "\n",
    "#function to conver chunks to embeddings using hugging face model\n",
    "def convert_chunks_to_embeddings(chunks: Dict[str, List[str]], embeddings_dir: str = \"embeddings\") -> Dict[str, List[np.ndarray]]:\n",
    "    if not os.path.exists(embeddings_dir):\n",
    "        os.makedirs(embeddings_dir)\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    for filename, chunked_texts in chunks.items():\n",
    "        embedding_file = os.path.join(embeddings_dir, f\"{filename}_embeddings.pkl\")\n",
    "\n",
    "        if os.path.exists(embedding_file):\n",
    "            # Load existing embeddings\n",
    "            with open(embedding_file, 'rb') as f:\n",
    "                embeddings[filename] = pickle.load(f)\n",
    "        else:\n",
    "            file_embeddings = []\n",
    "            for chunk in chunked_texts:\n",
    "                inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "                # Generate embeddings\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    embeddings_tensor = outputs.last_hidden_state\n",
    "\n",
    "                # Mean pooling to get sentence embeddings\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                embeddings_tensor = torch.sum(embeddings_tensor * attention_mask.unsqueeze(-1), dim=1) / torch.clamp(attention_mask.sum(dim=1, keepdim=True), min=1e-9)\n",
    "\n",
    "                # Convert to numpy array and store\n",
    "                embedding = embeddings_tensor.numpy()\n",
    "                file_embeddings.append(embedding)\n",
    "\n",
    "            embeddings[filename] = file_embeddings\n",
    "\n",
    "            with open(embedding_file, 'wb') as f:\n",
    "                pickle.dump(file_embeddings, f)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#function comparing the embeddings and returning top3 relevant matches \n",
    "def find_top3_relevant_chunks(question_embeddings: np.ndarray, chunks_embeddings: Dict[str, List[np.ndarray]], chunk_texts: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    top3_relevant_chunks = {}\n",
    "\n",
    "    for filename, embeddings in chunks_embeddings.items():\n",
    "        flattened_embeddings = [emb.squeeze(axis=0) for emb in embeddings]\n",
    "        flattened_embeddings = np.vstack(flattened_embeddings)\n",
    "\n",
    "        # Calculate cosine similarity between question embeddings and chunks embeddings\n",
    "        similarities = cosine_similarity([question_embeddings], flattened_embeddings).flatten()\n",
    "\n",
    "        sorted_indices = np.argsort(similarities)[::-1]  # Sort in descending order\n",
    "        top3_chunks = [chunk_texts[filename][idx] for idx in sorted_indices[:3]]\n",
    "\n",
    "        top3_relevant_chunks[filename] = top3_chunks\n",
    "\n",
    "    return top3_relevant_chunks\n",
    "\n",
    "#function to generate the prompt and retrieving the response from the llama3 model\n",
    "def prepare_and_get_response(question: str, top3_relevant_chunks: Dict[str, List[str]]) -> str:\n",
    "    prompt_template = f\"\"\"\n",
    "      You are a helpful Assistant who answers to users questions based on multiple contexts given to you.\n",
    "\n",
    "      Keep your answer short and to the point.\n",
    "\n",
    "      The evidence are the context of the pdf extract with metadata.\n",
    "\n",
    "      Carefully focus on the metadata specially 'filename' and 'page' whenever answering.\n",
    "\n",
    "      Make sure to add filename and page number at the end of sentence you are citing to.\n",
    "\n",
    "      Reply \"Not applicable\" if text is irrelevant.\n",
    "\n",
    "      The file content is:\n",
    "    \"\"\"\n",
    "\n",
    "    # Add each filename's top 3 chunks to the prompt template\n",
    "    for fname, chunks in top3_relevant_chunks.items():\n",
    "        prompt_template += f\"\\n\\nFilename: {fname}\\n\"\n",
    "        prompt_template += \"\\n\".join([f\"- {chunk}\" for chunk in chunks])\n",
    "\n",
    "    # Generate response using ollama.chat method\n",
    "    response = ollama.chat(\n",
    "        model='llama3',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': prompt_template\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': question\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "#main function hosting the app using streamlit\n",
    "def main():\n",
    "    st.set_page_config(layout=\"wide\")\n",
    "    st.title(\"RAG-Powered Llama AI\")\n",
    "\n",
    "    st.sidebar.header(\"Upload Documents\")\n",
    "    uploaded_files = st.sidebar.file_uploader(\"Upload your PDF, TXT, or MD files\", type=[\"pdf\", \"txt\", \"md\"], accept_multiple_files=True)\n",
    "\n",
    "    if uploaded_files:\n",
    "        filenames = [uploaded_file.name for uploaded_file in uploaded_files]\n",
    "        file_bytes = [BytesIO(uploaded_file.read()) for uploaded_file in uploaded_files]\n",
    "\n",
    "        parsed_contents = parse_folder(file_bytes, filenames)\n",
    "        chunks = split_text_into_chunks(parsed_contents)\n",
    "        embeddings = convert_chunks_to_embeddings(chunks)\n",
    "\n",
    "        if 'history' not in st.session_state:\n",
    "            st.session_state.history = []\n",
    "\n",
    "        if st.session_state.history:\n",
    "            st.subheader(\"History\")\n",
    "            for entry in st.session_state.history:\n",
    "                st.write(f\"**Question:** {entry['question']}\")\n",
    "                st.write(f\"**Response:** {entry['response']}\")\n",
    "                st.write(\"---\")\n",
    "\n",
    "        with st.form(key='question_form', clear_on_submit=True):\n",
    "            question = st.text_input(\"Enter your question:\", key=\"question_input\")\n",
    "            submit_button = st.form_submit_button(label='Submit')\n",
    "\n",
    "            if submit_button and question:\n",
    "                # Get question embeddings\n",
    "                inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    question_embeddings_tensor = outputs.last_hidden_state\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                question_embeddings_tensor = torch.sum(question_embeddings_tensor * attention_mask.unsqueeze(-1), dim=1) / torch.clamp(attention_mask.sum(dim=1, keepdim=True), min=1e-9)\n",
    "                question_embeddings = question_embeddings_tensor.numpy().squeeze(axis=0)\n",
    "\n",
    "                # Find top 3 relevant chunks\n",
    "                top3_relevant_chunks = find_top3_relevant_chunks(question_embeddings, embeddings, chunks)\n",
    "\n",
    "                response = prepare_and_get_response(question, top3_relevant_chunks)\n",
    "\n",
    "                st.session_state.history.append({\n",
    "                    'question': question,\n",
    "                    'response': response\n",
    "                })\n",
    "\n",
    "                # Display response word by word\n",
    "                response_placeholder = st.empty()\n",
    "                words = response.split()\n",
    "                for i in range(len(words)):\n",
    "                    response_placeholder.write(\" \".join(words[:i + 1]))\n",
    "                    time.sleep(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com\n",
    "! streamlit run app.py & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
